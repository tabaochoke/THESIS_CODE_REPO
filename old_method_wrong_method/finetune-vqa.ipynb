{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7667476,"sourceType":"datasetVersion","datasetId":4471674}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"Graphcore/vqa\" , split=\"validation[:200]\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-24T04:29:56.158951Z","iopub.execute_input":"2024-02-24T04:29:56.159887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Nghich data xem thử data như thế nào.\n2. label2id và id2label.\n3. replace string answer to its id.\n4. processing data. (tokenize word and image)\n5. hugging face trainer to train model.\n6. Run inference.\n7. run generate blip.","metadata":{}},{"cell_type":"code","source":"dataset[15]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image \nImage.open (dataset[15]['image_id'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[0]['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## label2id and id2label","metadata":{}},{"cell_type":"code","source":"label = list ( set ( [ids  for item in dataset for ids in item['label']['ids']] ) ) \nlabel2id = { label[i] : i for i in range (len (label)) }\nid2label = { v : k for k , v in label2id.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[0]['label']['ids']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## remove cols","metadata":{}},{"cell_type":"code","source":"dataset = dataset.remove_columns([\"question_type\", \"question_id\" , \"answer_type\"])\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# convert label","metadata":{}},{"cell_type":"code","source":"def convert_ids (sample) :\n    sample['label']['ids'] = [label2id [ids] for ids in sample['label']['ids']]\n    return sample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"convert_id_data = dataset.map (convert_ids )\nflat_data = convert_id_data.flatten ()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flat_data.features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Processing data","metadata":{}},{"cell_type":"code","source":"# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n# image = Image.open(requests.get(url, stream=True).raw)\n# text = \"a bunch of [MASK] laying on a [MASK].\"\n\n# processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n# model = ViltForMaskedLM.from_pretrained(\"dandelin/vilt-b32-mlm\")\n\n# # prepare inputs\n# encoding = processor(image, text, return_tensors=\"pt\")\n\n# # forward pass\n# outputs = model(**encoding)\n\n# tl = len(re.findall(\"\\[MASK\\]\", text))\n# inferred_token = [text]\n\n# # gradually fill in the MASK tokens, one by one\n# with torch.no_grad():\n#     for i in range(tl):\n#         #tokenize input\n#         encoded = processor.tokenizer(inferred_token)\n#         # convert to tensor\n#         input_ids = torch.tensor(encoded.input_ids).to('cuda')\n#         # \n#         encoded = encoded[\"input_ids\"][0][1:-1]\n#         outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n#         mlm_logits = outputs.logits[0]  # shape (seq_len, vocab_size)\n#         # only take into account text features (minus CLS and SEP token)\n#         mlm_logits = mlm_logits[1 : input_ids.shape[1] - 1, :]\n#         mlm_values, mlm_ids = mlm_logits.softmax(dim=-1).max(dim=-1)\n#         # only take into account text\n#         mlm_values[torch.tensor(encoded) != 103] = 0\n#         select = mlm_values.argmax().item()\n#         encoded[select] = mlm_ids[select].item()\n#         inferred_token = [processor.decode(encoded)]\n\n# selected_token = \"\"\n# encoded = processor.tokenizer(inferred_token)\n# processor.decode(encoded.input_ids[0], skip_special_tokens=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'pixel_mask', 'labels']","metadata":{}},{"cell_type":"code","source":"from transformers import ViltProcessor, ViltForMaskedLM\nimport requests\nfrom PIL import Image\nimport re\nimport torch\nprocessor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n# encoding = processor(image, text, return_tensors=\"pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flat_data.features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def processing (samples) :\n    # extract variable\n    image_patchs =  samples['image_id']\n    question =  samples['question']\n    \n    label_id = samples ['label.ids']\n    label_weights = samples['label.weights']\n    \n    image = [Image.open (image_patch) for image_patch in image_patchs]\n    #Preprocess\n    encoding = processor(image, question, return_tensors=\"pt\" , truncation = True , padding = \"max_length\")\n    for k , v in encoding.items () :\n        encoding[k] = v.squeeze () \n        \n        \n    targets = [] \n    for id_s, scores in zip (label_id , label_weights) :\n        target = torch.zeros (len (label2id))\n        for id_, score in zip (id_s , scores) :\n            target[id_] = score\n        targets.append (target)\n    encoding['labels'] = targets\n    return encoding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef preprocess_data(examples):\n    image_paths = examples['image_id']\n    images = [Image.open(image_path) for image_path in image_paths]\n    texts = examples['question']    \n\n    encoding = processor(images, texts, padding=\"max_length\", truncation=True)\n\n    for k, v in encoding.items():\n          encoding[k] = v\n    targets = []\n\n    for labels, scores in zip(examples['label.ids'], examples['label.weights']):\n        target = [0] * len(id2label)\n\n        for label, score in zip(labels, scores):\n            target[label] = score\n        targets.append(target)\n\n    encoding[\"labels\"] = targets\n    \n#    print (encoding)\n    return encoding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flat_data.map(preprocess_data, batched=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DefaultDataCollator\n\ndata_collator = DefaultDataCollator()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import ViltForQuestionAnswering\n\nmodel = ViltForQuestionAnswering.from_pretrained(model_checkpoint, num_labels=len(id2label), id2label=id2label, label2id=label2id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nrepo_id = \"HelloWorld2307/ViLT\"\n\ntraining_args = TrainingArguments(\n    output_dir=repo_id,\n    per_device_train_batch_size=4,\n    num_train_epochs=20,\n    save_steps=200,\n    logging_steps=50,\n    learning_rate=5e-5,\n    save_total_limit=2,\n    remove_unused_columns=False,\n    push_to_hub=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data,\n    tokenizer=processor,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import ViltProcessor, ViltForQuestionAnswering\nimport requests\nfrom PIL import Image\n\n# prepare image + question\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ntext = \"How many cats are there?\"\n\nprocessor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\nmodel = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n\n# prepare inputs\nencoding = processor(image, text, return_tensors=\"pt\")\n\n# forward pass\noutputs = model(**encoding)\nlogits = outputs.logits\nidx = logits.argmax(-1).item()\nprint(\"Predicted answer:\", model.config.id2label[idx])","metadata":{"execution":{"iopub.status.busy":"2024-03-02T03:51:39.931889Z","iopub.execute_input":"2024-03-02T03:51:39.932719Z","iopub.status.idle":"2024-03-02T03:52:04.922627Z","shell.execute_reply.started":"2024-03-02T03:51:39.932683Z","shell.execute_reply":"2024-03-02T03:52:04.921681Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-02 03:51:49.548266: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-02 03:51:49.548363: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-02 03:51:49.676923: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/251 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"724926dd7375406f8286a132753029d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe34e8ec9aac48f38a7b93398c1485c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"512683d173c8480bad0dee0a654d17ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"692f7c8e385147c1b6a111f5bd343d64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"108862bdc53842e0a7cacc2575f927f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/136k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fecbcbecd9c84b45ba181ac6b8d6b16f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/470M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a4532b8b0c345a5b34770ebcecdc6ea"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"name":"stdout","text":"Predicted answer: 2\n","output_type":"stream"}]},{"cell_type":"code","source":"encoding","metadata":{"execution":{"iopub.status.busy":"2024-03-02T03:55:21.652640Z","iopub.execute_input":"2024-03-02T03:55:21.653429Z","iopub.status.idle":"2024-03-02T03:55:21.670378Z","shell.execute_reply.started":"2024-03-02T03:55:21.653390Z","shell.execute_reply":"2024-03-02T03:55:21.669296Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[ 101, 2129, 2116, 8870, 2024, 2045, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]]), 'pixel_values': tensor([[[[ 0.1059,  0.1294,  0.1373,  ..., -0.2000, -0.1922, -0.1922],\n          [ 0.0824,  0.1137,  0.1608,  ..., -0.2235, -0.1373, -0.2314],\n          [ 0.0980,  0.1765,  0.1529,  ..., -0.1922, -0.2078, -0.2235],\n          ...,\n          [ 0.8353,  0.8353,  0.8196,  ...,  0.4980,  0.4667,  0.4510],\n          [ 0.8118,  0.8039,  0.7882,  ...,  0.0353,  0.0667, -0.1137],\n          [ 0.8667,  0.9059,  0.7098,  ..., -0.3490, -0.4039, -0.4275]],\n\n         [[-0.8039, -0.8118, -0.8275,  ..., -0.9137, -0.8824, -0.9137],\n          [-0.8118, -0.8039, -0.7961,  ..., -0.9059, -0.8824, -0.8980],\n          [-0.7961, -0.7569, -0.7882,  ..., -0.8745, -0.8588, -0.9059],\n          ...,\n          [-0.2627, -0.2627, -0.2549,  ..., -0.5686, -0.5216, -0.5608],\n          [-0.3176, -0.3176, -0.2863,  ..., -0.7647, -0.7647, -0.8745],\n          [-0.2314, -0.1922, -0.3882,  ..., -0.8824, -0.8745, -0.8824]],\n\n         [[-0.5451, -0.4902, -0.4667,  ..., -0.6941, -0.6941, -0.7333],\n          [-0.5922, -0.6078, -0.5137,  ..., -0.7176, -0.6941, -0.7569],\n          [-0.6392, -0.4980, -0.5529,  ..., -0.7176, -0.7255, -0.7725],\n          ...,\n          [ 0.5451,  0.5373,  0.5216,  ...,  0.2314,  0.2627,  0.2235],\n          [ 0.6392,  0.6157,  0.5608,  ..., -0.3255, -0.3255, -0.4588],\n          [ 0.4667,  0.6000,  0.5451,  ..., -0.7490, -0.7176, -0.6941]]]]), 'pixel_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]]])}"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2024-03-02T04:03:42.561908Z","iopub.execute_input":"2024-03-02T04:03:42.562310Z","iopub.status.idle":"2024-03-02T04:03:42.568471Z","shell.execute_reply.started":"2024-03-02T04:03:42.562271Z","shell.execute_reply":"2024-03-02T04:03:42.567301Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model.vilt.embeddings.text_embeddings.add_module (module = nn.Linear (1520,3000) , name = \"dropout\")","metadata":{"execution":{"iopub.status.busy":"2024-03-02T04:17:38.051489Z","iopub.execute_input":"2024-03-02T04:17:38.052154Z","iopub.status.idle":"2024-03-02T04:17:38.067001Z","shell.execute_reply.started":"2024-03-02T04:17:38.052122Z","shell.execute_reply":"2024-03-02T04:17:38.065985Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"encoding['input_ids']","metadata":{"execution":{"iopub.status.busy":"2024-03-02T04:17:38.432870Z","iopub.execute_input":"2024-03-02T04:17:38.433734Z","iopub.status.idle":"2024-03-02T04:17:38.440307Z","shell.execute_reply.started":"2024-03-02T04:17:38.433699Z","shell.execute_reply":"2024-03-02T04:17:38.439313Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"tensor([[ 101, 2129, 2116, 8870, 2024, 2045, 1029,  102]])"},"metadata":{}}]},{"cell_type":"code","source":"model.vilt.embeddings.text_embeddings.forward (encoding['input_ids']).shape","metadata":{"execution":{"iopub.status.busy":"2024-03-02T04:17:39.896317Z","iopub.execute_input":"2024-03-02T04:17:39.896672Z","iopub.status.idle":"2024-03-02T04:17:39.905465Z","shell.execute_reply.started":"2024-03-02T04:17:39.896647Z","shell.execute_reply":"2024-03-02T04:17:39.904514Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 8, 1520])"},"metadata":{}}]},{"cell_type":"code","source":"model.vilt.embeddings.text_embeddings.","metadata":{"execution":{"iopub.status.busy":"2024-03-02T04:29:16.270237Z","iopub.execute_input":"2024-03-02T04:29:16.271110Z","iopub.status.idle":"2024-03-02T04:29:16.319552Z","shell.execute_reply.started":"2024-03-02T04:29:16.271077Z","shell.execute_reply":"2024-03-02T04:29:16.318498Z"},"trusted":true},"execution_count":47,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvilt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextEmbeddings\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'TextEmbeddings' object has no attribute 'TextEmbeddings'"],"ename":"AttributeError","evalue":"'TextEmbeddings' object has no attribute 'TextEmbeddings'","output_type":"error"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-03-02T04:07:53.612624Z","iopub.execute_input":"2024-03-02T04:07:53.612964Z","iopub.status.idle":"2024-03-02T04:07:53.621945Z","shell.execute_reply.started":"2024-03-02T04:07:53.612939Z","shell.execute_reply":"2024-03-02T04:07:53.620758Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"ViltForQuestionAnswering(\n  (vilt): ViltModel(\n    (embeddings): ViltEmbeddings(\n      (text_embeddings): TextEmbeddings(\n        (word_embeddings): Embedding(30522, 768)\n        (position_embeddings): Embedding(40, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n        (Linear): Linear(in_features=768, out_features=1520, bias=True)\n      )\n      (patch_embeddings): ViltPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n      )\n      (token_type_embeddings): Embedding(2, 768)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViltEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViltLayer(\n          (attention): ViltAttention(\n            (attention): ViltSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViltSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViltIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViltOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViltPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=768, out_features=1536, bias=True)\n    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n    (2): GELU(approximate='none')\n    (3): Linear(in_features=1536, out_features=3129, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModel\nmodel = AutoModel.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")","metadata":{"execution":{"iopub.status.busy":"2024-03-02T04:40:47.655211Z","iopub.execute_input":"2024-03-02T04:40:47.655877Z","iopub.status.idle":"2024-03-02T04:40:49.305630Z","shell.execute_reply.started":"2024-03-02T04:40:47.655847Z","shell.execute_reply":"2024-03-02T04:40:49.304649Z"},"trusted":true},"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46f54a6341b74233bc4891d5bb1be596"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28dae81187a046d284e6adab20fcf449"}},"metadata":{}}]},{"cell_type":"code","source":"model.embeddings.add_module (module = nn.Sequential (nn.Linear (768 , 1520) , nn.Dropout (0.1)) , name = \"dropout\")","metadata":{"execution":{"iopub.status.busy":"2024-03-02T04:45:48.721621Z","iopub.execute_input":"2024-03-02T04:45:48.722415Z","iopub.status.idle":"2024-03-02T04:45:48.739279Z","shell.execute_reply.started":"2024-03-02T04:45:48.722387Z","shell.execute_reply":"2024-03-02T04:45:48.738517Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"model.embeddings.forward(encoding['input_ids']).shape","metadata":{"execution":{"iopub.status.busy":"2024-03-02T04:46:36.090095Z","iopub.execute_input":"2024-03-02T04:46:36.091095Z","iopub.status.idle":"2024-03-02T04:46:36.101321Z","shell.execute_reply.started":"2024-03-02T04:46:36.091060Z","shell.execute_reply":"2024-03-02T04:46:36.100297Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 8, 1520])"},"metadata":{}}]},{"cell_type":"code","source":"model.embeddings.get_submodule","metadata":{"execution":{"iopub.status.busy":"2024-03-02T04:52:41.994862Z","iopub.execute_input":"2024-03-02T04:52:41.995192Z","iopub.status.idle":"2024-03-02T04:52:42.001538Z","shell.execute_reply.started":"2024-03-02T04:52:41.995167Z","shell.execute_reply":"2024-03-02T04:52:42.000600Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"<bound method Module.get_submodule of Embeddings(\n  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n  (position_embeddings): Embedding(512, 768)\n  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  (dropout): Sequential(\n    (0): Linear(in_features=768, out_features=1520, bias=True)\n    (1): Dropout(p=0.1, inplace=False)\n  )\n)>"},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-03-02T04:45:50.804217Z","iopub.execute_input":"2024-03-02T04:45:50.804886Z","iopub.status.idle":"2024-03-02T04:45:50.811161Z","shell.execute_reply.started":"2024-03-02T04:45:50.804857Z","shell.execute_reply":"2024-03-02T04:45:50.810358Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"DistilBertModel(\n  (embeddings): Embeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Sequential(\n      (0): Linear(in_features=768, out_features=1520, bias=True)\n      (1): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (transformer): Transformer(\n    (layer): ModuleList(\n      (0-5): 6 x TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          (activation): GELUActivation()\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}